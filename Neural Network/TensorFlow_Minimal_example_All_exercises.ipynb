{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression: Exploring Model Parameters with TensorFlow\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we explore the effects of various parameters on a simple linear regression model using TensorFlow. The goal is to understand how changes in the number of observations, learning rate, and loss function impact the model's performance. We will also investigate the robustness of the model when using different target functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\pedro\\anaconda3\\envs\\notebook_env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We must always import the relevant libraries for our problem at hand. NumPy and TensorFlow are required for this example.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "We generate synthetic data based on a linear relationship with added noise. The target function used for generating the targets is:\n",
    "\n",
    "$$ \\ f(x, z) = 2x - 3z + 5 + \\text{noise}\\ $$\n",
    "\n",
    "This function serves as our ground truth for evaluating the model's performance.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "In this section, we will conduct a series of experiments to evaluate how different parameters affect the performance of our linear regression model. The following exercises will guide our exploration:\n",
    "\n",
    "1. **Change the Number of Observations**: We will test the model with two different observation counts: **1,000** and **10,000**. This will help us understand how the size of the dataset impacts model accuracy and training stability.\n",
    "\n",
    "2. **Experiment with Learning Rates**: We will experiment with various learning rates, specifically **0.0001**, **0.001**, **0.01**, **0.1**, and **1**. By observing the effects of these learning rates on convergence speed and final accuracy, we can determine which rates yield the best performance.\n",
    "\n",
    "3. **Change the Loss Function**: We will compare two loss functions:\n",
    "   - **Mean Squared Error (MSE)**: A common loss function for regression tasks.\n",
    "   - **Huber Loss**: This loss function is more robust to outliers than MSE, making it a valuable alternative in scenarios where data may contain anomalies.\n",
    "\n",
    "4. **Evaluate Model Performance**: For each configuration tested (combining observations, learning rates, and loss functions), we will track key performance metrics including:\n",
    "   - Estimated weights for the model\n",
    "   - Bias term\n",
    "   - R-squared values to assess goodness-of-fit\n",
    "   - Final loss metrics at the end of training\n",
    "\n",
    "By systematically varying these parameters and analyzing their effects, we aim to gain insights into optimizing linear regression models for better predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Observations=1000, Learning Rate=0.01, Loss Function=mean_squared_error\n",
      "Epoch 1/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 31.8502\n",
      "Epoch 2/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.7384\n",
      "Epoch 3/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.5399\n",
      "Epoch 4/7\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6605\n",
      "Epoch 5/7\n",
      "32/32 [==============================] - 0s 973us/step - loss: 0.4387\n",
      "Epoch 6/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3811\n",
      "Epoch 7/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3536\n",
      "32/32 [==============================] - 0s 964us/step\n",
      "Training: Observations=1000, Learning Rate=0.01, Loss Function=huber_loss\n",
      "Epoch 1/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 14.6925\n",
      "Epoch 2/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.8292\n",
      "Epoch 3/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 4.3530\n",
      "Epoch 4/7\n",
      "32/32 [==============================] - 0s 972us/step - loss: 3.8431\n",
      "Epoch 5/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.5150\n",
      "Epoch 6/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 3.1886\n",
      "Epoch 7/7\n",
      "32/32 [==============================] - 0s 958us/step - loss: 2.8681\n",
      "32/32 [==============================] - 0s 933us/step\n",
      "Training: Observations=1000, Learning Rate=0.001, Loss Function=mean_squared_error\n",
      "Epoch 1/7\n",
      "32/32 [==============================] - 0s 955us/step - loss: 136.6260\n",
      "Epoch 2/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 22.3761\n",
      "Epoch 3/7\n",
      "32/32 [==============================] - 0s 963us/step - loss: 18.2435\n",
      "Epoch 4/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 16.0627\n",
      "Epoch 5/7\n",
      "32/32 [==============================] - 0s 958us/step - loss: 14.1712\n",
      "Epoch 6/7\n",
      "32/32 [==============================] - 0s 965us/step - loss: 12.5085\n",
      "Epoch 7/7\n",
      "32/32 [==============================] - 0s 981us/step - loss: 11.0542\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "Training: Observations=1000, Learning Rate=0.001, Loss Function=huber_loss\n",
      "Epoch 1/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 17.0861\n",
      "Epoch 2/7\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 16.4082\n",
      "Epoch 3/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 15.7360\n",
      "Epoch 4/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 15.0716\n",
      "Epoch 5/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 14.4028\n",
      "Epoch 6/7\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 13.7402\n",
      "Epoch 7/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 13.0846\n",
      "32/32 [==============================] - 0s 533us/step\n",
      "Training: Observations=1000, Learning Rate=0.0001, Loss Function=mean_squared_error\n",
      "Epoch 1/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 359.8531\n",
      "Epoch 2/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 241.3516\n",
      "Epoch 3/7\n",
      "32/32 [==============================] - 0s 705us/step - loss: 164.6600\n",
      "Epoch 4/7\n",
      "32/32 [==============================] - 0s 923us/step - loss: 115.0631\n",
      "Epoch 5/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 82.9728\n",
      "Epoch 6/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 61.7262\n",
      "Epoch 7/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 48.3667\n",
      "32/32 [==============================] - 0s 709us/step\n",
      "Training: Observations=1000, Learning Rate=0.0001, Loss Function=huber_loss\n",
      "Epoch 1/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 16.8287\n",
      "Epoch 2/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 16.7606\n",
      "Epoch 3/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 16.6924\n",
      "Epoch 4/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 16.6243\n",
      "Epoch 5/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 16.5563\n",
      "Epoch 6/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 16.4881\n",
      "Epoch 7/7\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 16.4196\n",
      "32/32 [==============================] - 0s 768us/step\n",
      "Training: Observations=10000, Learning Rate=0.01, Loss Function=mean_squared_error\n",
      "Epoch 1/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 3.9784\n",
      "Epoch 2/7\n",
      "313/313 [==============================] - 0s 958us/step - loss: 0.3424\n",
      "Epoch 3/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3423\n",
      "Epoch 4/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3447\n",
      "Epoch 5/7\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 0.3415\n",
      "Epoch 6/7\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3433\n",
      "Epoch 7/7\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 0.3436\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Training: Observations=10000, Learning Rate=0.01, Loss Function=huber_loss\n",
      "Epoch 1/7\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 4.7385\n",
      "Epoch 2/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.7359\n",
      "Epoch 3/7\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 0.1752\n",
      "Epoch 4/7\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 0.1707\n",
      "Epoch 5/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1701\n",
      "Epoch 6/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1708\n",
      "Epoch 7/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1705\n",
      "313/313 [==============================] - 0s 808us/step\n",
      "Training: Observations=10000, Learning Rate=0.001, Loss Function=mean_squared_error\n",
      "Epoch 1/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 25.9372\n",
      "Epoch 2/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.5497\n",
      "Epoch 3/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.5391\n",
      "Epoch 4/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.6799\n",
      "Epoch 5/7\n",
      "313/313 [==============================] - 0s 976us/step - loss: 0.4349\n",
      "Epoch 6/7\n",
      "313/313 [==============================] - 0s 934us/step - loss: 0.3653\n",
      "Epoch 7/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3451\n",
      "313/313 [==============================] - 0s 934us/step\n",
      "Training: Observations=10000, Learning Rate=0.001, Loss Function=huber_loss\n",
      "Epoch 1/7\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 13.7663\n",
      "Epoch 2/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 7.8577\n",
      "Epoch 3/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 4.5788\n",
      "Epoch 4/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 3.9538\n",
      "Epoch 5/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 3.6339\n",
      "Epoch 6/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 3.3204\n",
      "Epoch 7/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 3.0079\n",
      "313/313 [==============================] - 0s 870us/step\n",
      "Training: Observations=10000, Learning Rate=0.0001, Loss Function=mean_squared_error\n",
      "Epoch 1/7\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 124.4341\n",
      "Epoch 2/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 21.7747\n",
      "Epoch 3/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 17.9206\n",
      "Epoch 4/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 15.8308\n",
      "Epoch 5/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 14.0066\n",
      "Epoch 6/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 12.3981\n",
      "Epoch 7/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 10.9781\n",
      "313/313 [==============================] - 0s 921us/step\n",
      "Training: Observations=10000, Learning Rate=0.0001, Loss Function=huber_loss\n",
      "Epoch 1/7\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 16.2549\n",
      "Epoch 2/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 15.5856\n",
      "Epoch 3/7\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 14.9206\n",
      "Epoch 4/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 14.2607\n",
      "Epoch 5/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 13.6062\n",
      "Epoch 6/7\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 12.9579\n",
      "Epoch 7/7\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 12.3167\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "All configurations:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observations</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Loss Function</th>\n",
       "      <th>Weight 1</th>\n",
       "      <th>Weight 2</th>\n",
       "      <th>Bias</th>\n",
       "      <th>R-squared</th>\n",
       "      <th>Losses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2.011706</td>\n",
       "      <td>-2.974261</td>\n",
       "      <td>4.929778</td>\n",
       "      <td>0.999141</td>\n",
       "      <td>[31.850217819213867, 4.73841667175293, 1.53985...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>huber_loss</td>\n",
       "      <td>2.053225</td>\n",
       "      <td>-2.827760</td>\n",
       "      <td>1.828379</td>\n",
       "      <td>0.975073</td>\n",
       "      <td>[14.692509651184082, 7.829202651977539, 4.3529...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1.982854</td>\n",
       "      <td>-3.025956</td>\n",
       "      <td>1.838679</td>\n",
       "      <td>0.975974</td>\n",
       "      <td>[136.62600708007812, 22.376117706298828, 18.24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>huber_loss</td>\n",
       "      <td>0.428362</td>\n",
       "      <td>-0.870542</td>\n",
       "      <td>0.026711</td>\n",
       "      <td>0.401435</td>\n",
       "      <td>[17.086088180541992, 16.40821075439453, 15.735...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1.563118</td>\n",
       "      <td>-2.339132</td>\n",
       "      <td>0.127479</td>\n",
       "      <td>0.901691</td>\n",
       "      <td>[359.8531494140625, 241.3516082763672, 164.659...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>huber_loss</td>\n",
       "      <td>0.075244</td>\n",
       "      <td>-0.121668</td>\n",
       "      <td>0.025911</td>\n",
       "      <td>0.016796</td>\n",
       "      <td>[16.828685760498047, 16.76061248779297, 16.692...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2.012729</td>\n",
       "      <td>-2.978918</td>\n",
       "      <td>5.003569</td>\n",
       "      <td>0.999188</td>\n",
       "      <td>[3.9784374237060547, 0.3424140512943268, 0.342...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>huber_loss</td>\n",
       "      <td>1.986225</td>\n",
       "      <td>-2.999171</td>\n",
       "      <td>5.013268</td>\n",
       "      <td>0.999222</td>\n",
       "      <td>[4.738548755645752, 0.7358721494674683, 0.1751...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1.997439</td>\n",
       "      <td>-2.995831</td>\n",
       "      <td>4.940317</td>\n",
       "      <td>0.999205</td>\n",
       "      <td>[25.937244415283203, 4.549702167510986, 1.5390...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>huber_loss</td>\n",
       "      <td>1.919859</td>\n",
       "      <td>-2.838318</td>\n",
       "      <td>1.653150</td>\n",
       "      <td>0.970439</td>\n",
       "      <td>[13.766300201416016, 7.857680320739746, 4.5788...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1.999336</td>\n",
       "      <td>-3.000010</td>\n",
       "      <td>1.835967</td>\n",
       "      <td>0.976301</td>\n",
       "      <td>[124.43405151367188, 21.774721145629883, 17.92...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>huber_loss</td>\n",
       "      <td>0.499972</td>\n",
       "      <td>-0.966550</td>\n",
       "      <td>0.142108</td>\n",
       "      <td>0.452232</td>\n",
       "      <td>[16.25486183166504, 15.585589408874512, 14.920...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Observations  Learning Rate       Loss Function  Weight 1  Weight 2  \\\n",
       "0           1000         0.0100  mean_squared_error  2.011706 -2.974261   \n",
       "1           1000         0.0100          huber_loss  2.053225 -2.827760   \n",
       "2           1000         0.0010  mean_squared_error  1.982854 -3.025956   \n",
       "3           1000         0.0010          huber_loss  0.428362 -0.870542   \n",
       "4           1000         0.0001  mean_squared_error  1.563118 -2.339132   \n",
       "5           1000         0.0001          huber_loss  0.075244 -0.121668   \n",
       "6          10000         0.0100  mean_squared_error  2.012729 -2.978918   \n",
       "7          10000         0.0100          huber_loss  1.986225 -2.999171   \n",
       "8          10000         0.0010  mean_squared_error  1.997439 -2.995831   \n",
       "9          10000         0.0010          huber_loss  1.919859 -2.838318   \n",
       "10         10000         0.0001  mean_squared_error  1.999336 -3.000010   \n",
       "11         10000         0.0001          huber_loss  0.499972 -0.966550   \n",
       "\n",
       "        Bias  R-squared                                             Losses  \n",
       "0   4.929778   0.999141  [31.850217819213867, 4.73841667175293, 1.53985...  \n",
       "1   1.828379   0.975073  [14.692509651184082, 7.829202651977539, 4.3529...  \n",
       "2   1.838679   0.975974  [136.62600708007812, 22.376117706298828, 18.24...  \n",
       "3   0.026711   0.401435  [17.086088180541992, 16.40821075439453, 15.735...  \n",
       "4   0.127479   0.901691  [359.8531494140625, 241.3516082763672, 164.659...  \n",
       "5   0.025911   0.016796  [16.828685760498047, 16.76061248779297, 16.692...  \n",
       "6   5.003569   0.999188  [3.9784374237060547, 0.3424140512943268, 0.342...  \n",
       "7   5.013268   0.999222  [4.738548755645752, 0.7358721494674683, 0.1751...  \n",
       "8   4.940317   0.999205  [25.937244415283203, 4.549702167510986, 1.5390...  \n",
       "9   1.653150   0.970439  [13.766300201416016, 7.857680320739746, 4.5788...  \n",
       "10  1.835967   0.976301  [124.43405151367188, 21.774721145629883, 17.92...  \n",
       "11  0.142108   0.452232  [16.25486183166504, 15.585589408874512, 14.920...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_model(observations, learning_rate, loss_function, epochs=20, batch_size=32):\n",
    "    # Generate data\n",
    "    xs = np.random.uniform(low=-10, high=10, size=(observations, 1))\n",
    "    zs = np.random.uniform(-10, 10, (observations, 1))\n",
    "    generated_inputs = np.column_stack((xs, zs))\n",
    "    noise = np.random.uniform(-1, 1, (observations, 1))\n",
    "    generated_targets = 2 * xs - 3 * zs + 5 + noise\n",
    "\n",
    "    # Define the model\n",
    "\n",
    "    # Declare a variable where we will store the input size of our model\n",
    "    # It should be equal to the number of variables you have\n",
    "    input_size = 2\n",
    "    # Declare the output size of the model\n",
    "    # It should be equal to the number of outputs you've got (for regressions that's usually 1)\n",
    "    output_size = 1\n",
    "    \n",
    "    # Outline the model\n",
    "    # We lay out the model in 'Sequential'\n",
    "    # Note that there are no calculations involved - we are just describing our network\n",
    "    model = tf.keras.Sequential([\n",
    "                                # Each 'layer' is listed here\n",
    "                                # The method 'Dense' indicates, our mathematical operation to be (xw + b)\n",
    "                                tf.keras.layers.Dense(output_size,\n",
    "                                # there are extra arguments you can include to customize your model\n",
    "                                # in our case we are just trying to create a solution that is \n",
    "                                # as close as possible to our NumPy model\n",
    "                                kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.1, maxval=0.1),\n",
    "                                bias_initializer=tf.keras.initializers.RandomUniform(minval=-0.1, maxval=0.1)\n",
    "                                )\n",
    "                                ])\n",
    "\n",
    "    # We can also define a custom optimizer, where we can specify the learning rate\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    \n",
    "    # Note that sometimes you may also need a custom loss function \n",
    "    # That's much harder to implement and won't be covered in this course though\n",
    "\n",
    "    # 'compile' is the place where you select and indicate the optimizers and the loss\n",
    "    model.compile(optimizer=optimizer, loss=loss_function)\n",
    "\n",
    "    # finally we fit the model, indicating the inputs and targets\n",
    "    # if they are not otherwise specified the number of epochs will be 1 (a single epoch of training), \n",
    "    # so the number of epochs is 'kind of' mandatory, too\n",
    "    # we can play around with verbose; we prefer verbose=2\n",
    "    history = model.fit(generated_inputs, generated_targets, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    # Extract weights and bias\n",
    "    weights, bias = model.layers[0].get_weights()\n",
    "\n",
    "    # We can predict new values in order to actually make use of the model\n",
    "    # Sometimes it is useful to round the values to be able to read the output\n",
    "    # Usually we use this method on NEW DATA, rather than our original training data\n",
    "    predictions = model.predict(generated_inputs)\n",
    "\n",
    "    # Calculate R-squared\n",
    "    ss_res = np.sum((generated_targets - predictions) ** 2)\n",
    "    ss_tot = np.sum((generated_targets - np.mean(generated_targets)) ** 2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "    return weights, bias, r_squared, history.history['loss']  # Loss history per epoch\n",
    "\n",
    "# Parameters for experiments\n",
    "observations_list = [1000, 10000]\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "loss_functions = ['mean_squared_error', 'huber_loss']\n",
    "\n",
    "results = []\n",
    "\n",
    "for obs in observations_list:\n",
    "    for lr in learning_rates:\n",
    "        for loss in loss_functions:\n",
    "            print(f\"Training: Observations={obs}, Learning Rate={lr}, Loss Function={loss}\")\n",
    "            weights, bias, r_squared, losses = train_model(obs, lr, loss, epochs=7)  # Reduced epochs\n",
    "            results.append({\n",
    "                'Observations': obs,\n",
    "                'Learning Rate': lr,\n",
    "                'Loss Function': loss,\n",
    "                'Weight 1': weights[0][0],\n",
    "                'Weight 2': weights[1][0],\n",
    "                'Bias': bias[0],\n",
    "                'R-squared': r_squared,\n",
    "                'Losses': losses\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"All configurations:\")\n",
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### All Configurations\n",
    "\n",
    "We train the model across all combinations of observations, learning rates, and loss functions. The results are compiled into a DataFrame for easier analysis.\n",
    "\n",
    "### Best Configuration\n",
    "\n",
    "After evaluating all configurations, we identify the best-performing setup based on the highest R-squared value. This configuration provides insights into which parameters yield optimal results in terms of accuracy and stability.\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "The following metrics are recorded for each configuration:\n",
    "- Observations\n",
    "- Learning Rate\n",
    "- Loss Function\n",
    "- Estimated Weights\n",
    "- Bias\n",
    "- R-squared Value\n",
    "- Final Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best configuration:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Observations                                                 10000\n",
       "Learning Rate                                                 0.01\n",
       "Loss Function                                           huber_loss\n",
       "Weight 1                                                  1.986225\n",
       "Weight 2                                                 -2.999171\n",
       "Bias                                                      5.013268\n",
       "R-squared                                                 0.999222\n",
       "Losses           [4.738548755645752, 0.7358721494674683, 0.1751...\n",
       "Name: 7, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the best configuration\n",
    "best_config = results_df.loc[results_df['R-squared'].idxmax()]\n",
    "\n",
    "print(\"\\nBest configuration:\")\n",
    "best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 4.8005\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.7656\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1744\n",
      "Epoch 4/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1695\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1692\n",
      "Epoch 6/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1691\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1689\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - 0s 962us/step - loss: 0.1695\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1692\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1693\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1692\n",
      "Epoch 12/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1695\n",
      "Epoch 13/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1697\n",
      "Epoch 14/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1692\n",
      "Epoch 15/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1694\n",
      "Epoch 16/50\n",
      "313/313 [==============================] - 0s 997us/step - loss: 0.1688\n",
      "Epoch 17/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1691\n",
      "Epoch 18/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1691\n",
      "Epoch 19/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1689\n",
      "Epoch 20/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1689\n",
      "Epoch 21/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1694\n",
      "Epoch 22/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1693\n",
      "Epoch 23/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1690\n",
      "Epoch 24/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1695\n",
      "Epoch 25/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1692\n",
      "Epoch 26/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1693\n",
      "Epoch 27/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1693\n",
      "Epoch 28/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1690\n",
      "Epoch 29/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1688\n",
      "Epoch 30/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1694\n",
      "Epoch 31/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1692\n",
      "Epoch 32/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1691\n",
      "Epoch 33/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1692\n",
      "Epoch 34/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1689\n",
      "Epoch 35/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1698\n",
      "Epoch 36/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1697\n",
      "Epoch 37/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1694\n",
      "Epoch 38/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1688\n",
      "Epoch 39/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1698\n",
      "Epoch 40/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1696\n",
      "Epoch 41/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1693\n",
      "Epoch 42/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1697\n",
      "Epoch 43/50\n",
      "313/313 [==============================] - 0s 959us/step - loss: 0.1692\n",
      "Epoch 44/50\n",
      "313/313 [==============================] - 0s 935us/step - loss: 0.1691\n",
      "Epoch 45/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1693\n",
      "Epoch 46/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1691\n",
      "Epoch 47/50\n",
      "313/313 [==============================] - 0s 952us/step - loss: 0.1690\n",
      "Epoch 48/50\n",
      "313/313 [==============================] - 0s 953us/step - loss: 0.1691\n",
      "Epoch 49/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1687\n",
      "Epoch 50/50\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1692\n",
      "313/313 [==============================] - 0s 835us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0EAAAHWCAYAAACxAYILAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABENUlEQVR4nO3deXhU9fn+8XuyTZLJQsIaZBWRsFdRMF8BUSJIqQVEcaEVsBXFgCKlWuoCWBH3n1YRrajUBUG0qEURQVksiggIAkVUChrZUSGQQAKZz++PZA4ZMglJmJlzkrxf1zUXmTMnmWdmTobc85zzHJcxxggAAAAAaokIuwsAAAAAgHAiBAEAAACoVQhBAAAAAGoVQhAAAACAWoUQBAAAAKBWIQQBAAAAqFUIQQAAAABqFUIQAAAAgFqFEAQAAACgViEEAQCCYunSpXK5XHrzzTdDfl+//vWvdeONN4b8fmqiw4cP649//KMaNWokl8ulsWPHavv27XK5XJo5c6bd5Z0Wl8ulSZMmhfQ+LrjgAt1xxx0hvQ8AoUcIAhBUM2fOlMvl0urVq+0upUJWrFihQYMGqWHDhnK73WrRooVuuukm/fDDD3aXVoovZJR1mT17tt0lhsWKFSv04Ycf6s4777SWBXpuUlNTdcEFF+i1114LaT0PPPCA3n777Up9T05OjiZPnqzOnTsrISFBcXFx6tChg+68807t3LkzNIUWe+CBBzRz5kyNGjVKr7zyin7/+9+H9P6C7f333w950CnPnXfeqWnTpmn37t221QDg9EXZXQAA2OWpp57SbbfdpjPPPFNjxoxRWlqaNm/erBkzZmjOnDl6//339X//9392l1nKrbfeqvPPP7/U8oyMDBuqCb9HHnlEvXv31llnnVXqtpLPzU8//aQ5c+bod7/7nQ4cOKCsrKyQ1PPAAw/oyiuv1MCBAyu0/v/+9z9lZmbqhx9+0FVXXaWRI0cqJiZGX331lV544QXNmzdP33zzTUhqlaSPP/5YF1xwgSZOnGgtM8boyJEjio6ODtn9Bsv777+vadOmBQxCR44cUVRUaP+0GTBggJKSkvTMM8/ovvvuC+l9AQgdQhCAWmnFihUaO3asunfvrg8++EDx8fHWbaNGjdKFF16oK6+8Ups2bVJKSkrY6srNzZXH4yl3nR49eujKK68MU0XOsnfvXr333nt69tlnA95+8nMzatQonXnmmZo1a1bIQlBlHD9+XFdccYX27NmjpUuXqnv37n63T5kyRQ899FBIa9i7d6/atWvnt8zlcik2Njak91uWimzzFRWOxxAREaErr7xSL7/8siZPniyXyxXy+wQQfOwOB8AWX375pfr166ekpCQlJCSod+/eWrlypd86x44d0+TJk9W6dWvFxsaqbt266t69uxYtWmSts3v3bo0YMUJNmjSR2+1WWlqaBgwYoO3bt5d7/3/729/kcrn0z3/+0y8ASVKrVq308MMPa9euXXruueckSY8++qhcLpe+//77Uj9rwoQJiomJ0S+//GIt+/zzz3XZZZcpOTlZ8fHxuuiii7RixQq/75s0aZJcLpf++9//6rrrrlNKSkqpP4qryuVyafTo0XrttdfUpk0bxcbGqkuXLlq+fHmpdSvyWkjSgQMHdPvtt6tFixZyu91q0qSJrr/+eu3fv99vPa/XqylTpqhJkyaKjY1V79699d133/mt8+2332rw4MFq1KiRYmNj1aRJE11zzTU6ePBguY/rvffe0/Hjx5WZmVmh5yEmJkYpKSkBuwOvvvqqunTpori4OKWmpuqaa65RdnZ2pep0uVzKzc3VP//5T2s3vOHDh5dZz1tvvaX169frrrvuCvhaJyUlacqUKX7L5s6da9VZr149/e53v9OOHTv81hk+fLgSEhK0Y8cODRw4UAkJCapfv77Gjx+vwsJCSSd2Gdy2bZvee+89q97t27eXeUzQ3Llz1a5dO8XGxqpDhw6aN2+ehg8frhYtWljr+H7u0qVL/b430M/01bl161b9+te/VmJiooYOHSpJ+uSTT3TVVVepWbNmcrvdatq0qW6//XYdOXLE7/unTZtmPfe+i0+gY4Iqsn37duNdsWKFxo0bp/r168vj8WjQoEHat29fqdfp0ksv1ffff69169aVug1A9UAnCEDYbdq0ST169FBSUpLuuOMORUdH67nnnlOvXr20bNkydevWTVJRSJg6dar++Mc/qmvXrsrJydHq1au1du1aXXrppZKkwYMHa9OmTRozZoxatGihvXv3atGiRfrhhx/8/lArKS8vTx999JF69Oihli1bBlzn6quv1siRIzV//nz95S9/0ZAhQ3THHXfojTfe0J///Ge/dd944w316dPH6hh9/PHH6tevn7p06aKJEycqIiJCL730ki655BJ98skn6tq1q9/3X3XVVWrdurUeeOABGWNO+fwdOnSoVPCQpLp16/r9Qbhs2TLNmTNHt956q9xut5555hlddtllWrVqlTp06FCp1+Lw4cPq0aOHNm/erBtuuEHnnnuu9u/fr3fffVc//vij6tWrZ93vgw8+qIiICI0fP14HDx7Uww8/rKFDh+rzzz+XJBUUFKhv377Kz8/XmDFj1KhRI+3YsUPz58/XgQMHlJycXOZj//TTT1W3bl01b978lM/Nzz//rFmzZmnjxo164YUX/NabMmWK7rnnHg0ZMkR//OMftW/fPj311FPq2bOnvvzyS9WpU6dCdb7yyivW9jly5EhJRSG6LO+++64kVfg4nJkzZ2rEiBE6//zzNXXqVO3Zs0dPPvmkVqxYYdXpU1hYqL59+6pbt2569NFHtXjxYj322GNq1aqVRo0apbZt2+qVV17R7bffriZNmuhPf/qTJKl+/foB/9B/7733dPXVV6tjx46aOnWqfvnlF/3hD3/QGWecUaHay3L8+HH17dtX3bt316OPPmp9CDF37lzl5eVp1KhRqlu3rlatWqWnnnpKP/74o+bOnStJuummm7Rz504tWrRIr7zyyinvq6Lbt8+YMWOUkpKiiRMnavv27XriiSc0evRozZkzx2+9Ll26SCrqKJ9zzjmn9XwAsIkBgCB66aWXjCTzxRdflLnOwIEDTUxMjNm6dau1bOfOnSYxMdH07NnTWta5c2fTv3//Mn/OL7/8YiSZRx55pFI1rlu3zkgyt912W7nrderUyaSmplrXMzIyTJcuXfzWWbVqlZFkXn75ZWOMMV6v17Ru3dr07dvXeL1ea728vDzTsmVLc+mll1rLJk6caCSZa6+9tkJ1L1myxEgq87Jr1y5rXd+y1atXW8u+//57ExsbawYNGmQtq+hrce+99xpJ5l//+lepunyP01df27ZtTX5+vnX7k08+aSSZDRs2GGOM+fLLL40kM3fu3Ao97pK6d+9e6jUo77mJiIgwU6ZM8Vt3+/btJjIystTyDRs2mKioKGt5Rev0eDxm2LBhFar/nHPOMcnJyRVat6CgwDRo0MB06NDBHDlyxFo+f/58I8nce++91rJhw4YZSea+++4rdX8nP1/Nmzcv9Xu1bds2I8m89NJL1rKOHTuaJk2amEOHDlnLli5daiSZ5s2bW8t8z/2SJUtO+TN9df7lL38p9Xjz8vJKLZs6dapxuVzm+++/t5ZlZWWZsv58kWQmTpxoXa/o9u1738rMzPT7vb399ttNZGSkOXDgQKn7iomJMaNGjQpYBwDnY3c4AGFVWFioDz/8UAMHDtSZZ55pLU9LS9N1112n//znP8rJyZEk1alTR5s2bdK3334b8GfFxcUpJiZGS5cu9dsV7VQOHTokSUpMTCx3vcTERKsWqag7tGbNGm3dutVaNmfOHLndbg0YMECStG7dOn377be67rrr9NNPP2n//v3av3+/cnNz1bt3by1fvlxer9fvfm6++eYK1y5J9957rxYtWlTqkpqa6rdeRkaG9Ym1JDVr1kwDBgzQwoULVVhYWKnX4q233lLnzp01aNCgUvWcfEzEiBEjFBMTY13v0aOHpKKBAJKsTs/ChQuVl5dXqcf+008/lXuMVsnnZs6cObr22mt111136cknn7TW+de//iWv16shQ4ZYr8/+/fvVqFEjtW7dWkuWLDntOsuSk5Nzyu3OZ/Xq1dq7d69uueUWv2Nd+vfvr/T0dL333nulvufkbalHjx7W814ZO3fu1IYNG3T99dcrISHBWn7RRRepY8eOlf55Jxs1alSpZXFxcdbXubm52r9/v/7v//5Pxhh9+eWXlb6PymzfPiNHjvTbnnv06KHCwsKAu8GmpKQE7MgCqB4IQQDCat++fcrLy1ObNm1K3da2bVt5vV7ruIz77rtPBw4c0Nlnn62OHTvqz3/+s7766itrfbfbrYceekgLFixQw4YN1bNnTz388MOnHF3r+yPUF4bKcujQIb8/WK+66ipFRERYu8YYYzR37lzreANJVmAbNmyY6tev73eZMWOG8vPzSx33UtYueWXp2LGjMjMzS11KBg9Jat26danvPfvss5WXl6d9+/ZV6rXYunWrtQvdqTRr1szvui+0+IJqy5YtNW7cOM2YMUP16tVT3759NW3atFMeD+RjytllsORzM2TIEL366qv6zW9+o7/85S/WLl/ffvutjDFq3bp1qddo8+bN2rt3b1DqDCQpKemU252P7w/vQK9Penp6qT/MY2NjVb9+fb9lKSkplfqA4OT7DjSBL9CyyoiKilKTJk1KLf/hhx80fPhwpaamWsc0XXTRRZJUpee8Mtu3z6m23ZKMMQxFAKoxQhAAx+rZs6e2bt2qF198UR06dNCMGTN07rnnasaMGdY6Y8eO1TfffKOpU6cqNjZW99xzj9q2bVvuJ8dnnXWWoqKi/ALVyfLz87Vlyxa/KVqNGzdWjx499MYbb0iSVq5cqR9++EFXX321tY6vy/PII48E7NYsWrTI75N1yf8T8JogMjIy4PKS4eWxxx7TV199pb/+9a86cuSIbr31VrVv314//vhjuT+7bt26lf6jvnfv3jp69KhWrVolqeg1crlc+uCDDwK+Pr5hGKdTZ1nS09N18ODBUn98B0NZz3uolRUEfAMZTuZ2uxUREVFq3UsvvVTvvfee7rzzTr399ttatGiRNVTh5O5pqFRk2/U5cOCA37FwAKoXQhCAsKpfv77i4+O1ZcuWUrd9/fXXioiIUNOmTa1lqampGjFihF5//XVlZ2erU6dOpaY/tWrVSn/605/04YcfauPGjSooKNBjjz1WZg0ej0cXX3yxli9fHnA3F6lo2EF+fr5+85vf+C2/+uqrtX79em3ZskVz5sxRfHy8Lr/8cr9apKJP/AN1azIzM8N2LpZAuxF+8803io+PtzofFX0tWrVqpY0bNwa1vo4dO+ruu+/W8uXL9cknn2jHjh1ljr72SU9P17Zt2yp1P8ePH5dUNNxBKnosxhi1bNky4OtzwQUXVKrOynQDfNvKq6++esp1fcMfAr0+W7ZsKXM4RDD4fvbJU/0CLfN1Sw4cOOC3vKzfrUA2bNigb775Ro899pjuvPNODRgwQJmZmWrcuHGpdSv6fFf2vaYyduzYoYKCArVt27ZK3w/AfoQgAGEVGRmpPn366J133vEbY71nzx7NmjVL3bt3t3Yt++mnn/y+NyEhQWeddZby8/MlFU15O3r0qN86rVq1UmJiorVOWe6++24ZYzR8+HC/EbyStG3bNt1xxx1KS0vTTTfd5Hfb4MGDFRkZqddff11z587Vb37zG79znHTp0kWtWrXSo48+av3RXVKgKVyh8tlnn2nt2rXW9ezsbL3zzjvq06ePIiMjK/VaDB48WOvXr9e8efNK3U95u6cFkpOTYwUTn44dOyoiIuKUr1tGRoZ++eWXSh3nMn/+fElS586dJUlXXHGFIiMjNXny5FK1G2Os7a6idXo8nlIBoCxXXnmlOnbsqClTpuizzz4rdfuhQ4d01113SZLOO+88NWjQQM8++6zf/S1YsECbN29W//79K3SfVdG4cWN16NBBL7/8st92vGzZMm3YsMFv3ebNmysyMrLU+PVnnnmmwvfn68CUfD2MMX7Hcvn4ft9O9ZxXZvuurDVr1kiSI0+mDKBiGJENICRefPFFffDBB6WW33bbbbr//vu1aNEide/eXbfccouioqL03HPPKT8/Xw8//LC1brt27dSrVy916dJFqampWr16td58802NHj1aUlFXo3fv3hoyZIjatWunqKgozZs3T3v27NE111xTbn09e/bUo48+qnHjxqlTp04aPny40tLS9PXXX+v555+X1+vV+++/X+og/AYNGujiiy/W448/rkOHDvntCicVnUhxxowZ6tevn9q3b68RI0bojDPO0I4dO7RkyRIlJSXp3//+d1WfVklF51M5OfxJUqdOndSpUyfreocOHdS3b1+/EdmSNHnyZGudir4Wf/7zn/Xmm2/qqquu0g033KAuXbro559/1rvvvqtnn33WChgV8fHHH2v06NG66qqrdPbZZ+v48eN65ZVXFBkZqcGDB5f7vf3791dUVJQWL15sjaQu67nx1bds2TJdc801Sk9Pl1QUlO+//35NmDBB27dv18CBA5WYmKht27Zp3rx5GjlypMaPH1/hOrt06aLFixfr8ccfV+PGjdWyZctSo5d9oqOj9a9//UuZmZnq2bOnhgwZogsvvFDR0dHatGmTZs2apZSUFE2ZMkXR0dF66KGHNGLECF100UW69tprrRHZLVq00O23317h57wqHnjgAQ0YMEAXXnihRowYoV9++UVPP/20OnTo4BeMkpOTddVVV+mpp56Sy+VSq1atNH/+fOvYqopIT09Xq1atNH78eO3YsUNJSUl66623Au766Bv2ceutt6pv376KjIws8/e9ott3ZS1atEjNmjVjPDZQndkxkg5AzeUbNVvWJTs72xhjzNq1a03fvn1NQkKCiY+PNxdffLH59NNP/X7W/fffb7p27Wrq1Klj4uLiTHp6upkyZYopKCgwxhizf/9+k5WVZdLT043H4zHJycmmW7du5o033qhwvcuXLzcDBgww9erVM9HR0aZZs2bmxhtvNNu3by/ze55//nkjySQmJvqNLi7pyy+/NFdccYWpW7eucbvdpnnz5mbIkCHmo48+stbxjcjet29fhWo91YjskqOBJZmsrCzz6quvmtatWxu3223OOeecUmOMjanYa2GMMT/99JMZPXq0OeOMM0xMTIxp0qSJGTZsmNm/f79ffSePlD55VPL//vc/c8MNN5hWrVqZ2NhYk5qaai6++GKzePHiCj0Pv/3tb03v3r1P+dzExMSU2mZKeuutt0z37t2Nx+MxHo/HpKenm6ysLLNly5ZK1fn111+bnj17mri4OCOpQuOyf/nlF3Pvvfeajh07mvj4eBMbG2s6dOhgJkyY4Dfq3Bhj5syZY8455xzjdrtNamqqGTp0qPnxxx/91hk2bJjxeDyl7se3jZVU0RHZxhgze/Zsk56ebtxut+nQoYN59913zeDBg016errfevv27TODBw828fHxJiUlxdx0001m48aNAUdkB6rTGGP++9//mszMTJOQkGDq1atnbrzxRrN+/fpSP+P48eNmzJgxpn79+sblcvk9vpN/D4yp2PZd1mj/QOO/CwsLTVpamrn77rsDPg4A1YPLmEruxwAAcDyXy6WsrCw9/fTTdpcSdJ988ol69eqlr7/+OuAEPITWr371K9WvX1+LFi2yuxRbvP3227ruuuu0detWpaWl2V0OgCrimCAAQLXSo0cP9enT57R2Z8KpHTt2rNQxUUuXLtX69evVq1cve4pygIceekijR48mAAHVHMcEAQCqnQULFthdQo23Y8cOZWZm6ne/+50aN26sr7/+Ws8++6waNWpU6RP81iSBBloAqH4IQQAAoJSUlBR16dJFM2bM0L59++TxeNS/f389+OCDqlu3rt3lAcBp4ZggAAAAALUKxwQBAAAAqFVsDUGTJk2Sy+Xyu/jO4wAAAAAAoWD7MUHt27fX4sWLretRURUvyev1aufOnUpMTJTL5QpFeQAAAACqAWOMDh06pMaNGysiovxej+0hKCoqSo0aNarS9+7cuVNNmzYNckUAAAAAqqvs7Gw1adKk3HVsD0HffvutGjdurNjYWGVkZGjq1Klq1qxZwHXz8/OVn59vXffNdMjOzlZSUlJY6gUAAADgPDk5OWratKkSExNPua6t0+EWLFigw4cPq02bNtq1a5cmT56sHTt2aOPGjQGLnzRpkiZPnlxq+cGDBwlBAAAAQC2Wk5Oj5OTkCmUDR43IPnDggJo3b67HH39cf/jDH0rdfnInyJf2CEEAAABA7VaZEGT77nAl1alTR2effba+++67gLe73W653e4wVwUAAACgJnHUeYIOHz6srVu3Ki0tze5SAAAAANRQtoag8ePHa9myZdq+fbs+/fRTDRo0SJGRkbr22mvtLAsAAABADWbr7nA//vijrr32Wv3000+qX7++unfvrpUrV6p+/fp2lgUAAACgBrM1BM2ePdvOuwcAAABQCznqmCAAAAAACDVCEAAAAIBahRAEAAAAoFYhBAEAAACoVQhBAAAAAGoVQhAAAACAWoUQBAAAAKBWIQQFyZT3/qs+/2+Z5n+10+5SAAAAAJSDEBQku3Py9c2ew9qbk293KQAAAADKQQgKEk9MpCQpr+C4zZUAAAAAKA8hKEjiY6IkSbkFhTZXAgAAAKA8hKAg8biLO0H5dIIAAAAAJyMEBQmdIAAAAKB6IAQFidUJ4pggAAAAwNEIQUFidYLy6QQBAAAATkYIChLfdLhcjgkCAAAAHI0QFCTxbo4JAgAAAKoDQlCQcJ4gAAAAoHogBAWJx80xQQAAAEB1QAgKEk/xYAQ6QQAAAICzEYKCJN4akV0or9fYXA0AAACAshCCgsTXCZKkI8fYJQ4AAABwKkJQkMRGR8jlKvo6l13iAAAAAMciBAWJy+U6cVwQwxEAAAAAxyIEBVG874SpdIIAAAAAxyIEBZFvTHYeJ0wFAAAAHIsQFERWJyifThAAAADgVISgIDpxriA6QQAAAIBTEYKCyHeuoMN0ggAAAADHIgQF0YnpcIQgAAAAwKkIQUF0Yjocu8MBAAAATkUICqIT0+HoBAEAAABORQgKIo/bNx2OThAAAADgVISgIIqPoRMEAAAAOB0hKIg8HBMEAAAAOB4hKIji3UyHAwAAAJyOEBREvhHZdIIAAAAA5yIEBZHvZKkcEwQAAAA4FyEoiE6cLJVOEAAAAOBUhKAgOnGyVDpBAAAAgFMRgoLIOlkqnSAAAADAsQhBQeQp0QkyxthcDQAAAIBACEFB5BuR7TXS0WNem6sBAAAAEAghKIjioiOtrzkuCAAAAHAmQlAQRUa4rCDEcUEAAACAMxGCgsw3HIFOEAAAAOBMhKAg83DCVAAAAMDRCEFBFl98wtRcdocDAAAAHIkQFGS+Mdl0ggAAAABnIgQFmW9MNp0gAAAAwJkIQUFGJwgAAABwNkJQkFnHBBXQCQIAAACciBAUZNZ0uHw6QQAAAIATEYKCjE4QAAAA4GyEoCDjmCAAAADA2QhBQcZ0OAAAAMDZCEFB5usE5XJMEAAAAOBIhKAgszpB7A4HAAAAOBIhKMgSfNPhGIwAAAAAOBIhKMis6XDsDgcAAAA4EiEoyDzFIYhOEAAAAOBMhKAgi3czGAEAAABwMkJQkJXsBBljbK4GAAAAwMkIQUHm6wQd9xoVFHptrgYAAADAyQhBQRYfHWl9nccJUwEAAADHcUwIevDBB+VyuTR27Fi7SzktUZERckcVPa2cKwgAAABwHkeEoC+++ELPPfecOnXqZHcpQeFxMyEOAAAAcCrbQ9Dhw4c1dOhQPf/880pJSbG7nKCIj2FCHAAAAOBUtoegrKws9e/fX5mZmadcNz8/Xzk5OX4XJ+JcQQAAAIBzRdl557Nnz9batWv1xRdfVGj9qVOnavLkySGu6vT5JsQdphMEAAAAOI5tnaDs7Gzddttteu211xQbG1uh75kwYYIOHjxoXbKzs0NcZdWc6AQRggAAAACnsa0TtGbNGu3du1fnnnuutaywsFDLly/X008/rfz8fEVGRvp9j9vtltvtDneplXbimCB2hwMAAACcxrYQ1Lt3b23YsMFv2YgRI5Senq4777yzVACqThLcdIIAAAAAp7ItBCUmJqpDhw5+yzwej+rWrVtqeXXjOyaIThAAAADgPLZPh6uJOCYIAAAAcC5bp8OdbOnSpXaXEBTxxSEolxHZAAAAgOPQCQoBT/HucHmMyAYAAAAchxAUAnSCAAAAAOciBIWA1QnimCAAAADAcQhBIWB1gpgOBwAAADgOISgEPDF0ggAAAACnIgSFQLybThAAAADgVISgEKATBAAAADgXISgE6AQBAAAAzkUICgFfJ6ig0KuC416bqwEAAABQEiEoBHzT4STpCOcKAgAAAByFEBQCMVERioksempzOS4IAAAAcBRCUIjEc8JUAAAAwJEIQSHi4YSpAAAAgCMRgkIkvng4ArvDAQAAAM5CCAoR35jsPDpBAAAAgKMQgkLEQycIAAAAcCRCUIj4xmTnMSIbAAAAcBRCUIh4iqfD5ebTCQIAAACchBAUInSCAAAAAGciBIUIxwQBAAAAzkQIChHfdDh2hwMAAACchRAUIr5OECOyAQAAAGchBIWIx9cJYnc4AAAAwFEIQSHimw7HYAQAAADAWQhBIeKbDscxQQAAAICzEIJCxMOIbAAAAMCRCEEhEu9mRDYAAADgRISgELE6QUyHAwAAAByFEBQi8ZwsFQAAAHAkQlCI+EZkHz3mVaHX2FwNAAAAAB9CUIj4OkGSlEc3CAAAAHAMQlCIuKMiFBnhksSEOAAAAMBJCEEh4nK5ThwXxLmCAAAAAMcgBIUQ5woCAAAAnIcQFEK+cwUdphMEAAAAOAYhKIROdIIIQQAAAIBTEIJCyOP2HRPE7nAAAACAUxCCQohOEAAAAOA8hKAQii8+YSqdIAAAAMA5CEEh5CkekU0nCAAAAHAOQlAIxRfvDpfLiGwAAADAMQhBIeQbjJDHiGwAAADAMQhBIUQnCAAAAHAeQlAIWZ0gjgkCAAAAHIMQFEJWJ4jpcAAAAIBjEIJCiOlwAAAAgPMQgkKI8wQBAAAAzkMICiE6QQAAAIDzEIJCyHdM0GE6QQAAAIBjEIJCKKF4dzg6QQAAAIBzEIJCKN4akV0or9fYXA0AAAAAiRAUUp7i3eEk6cgxdokDAAAAnIAQFEKx0RFyuYq+zmWXOAAAAMARCEEh5HK5rG5QHsMRAAAAAEcgBIVYfPGYbDpBAAAAgDMQgkLMY02IoxMEAAAAOAEhKMSsTlA+nSAAAADACQhBIWYdE0QnCAAAAHAEQlCI+c4VRCcIAAAAcAZCUIjRCQIAAACchRAUYkyHAwAAAJyFEBRivulw7A4HAAAAOAMhKMROTIdjdzgAAADACQhBIXbiPEF0ggAAAAAnsDUETZ8+XZ06dVJSUpKSkpKUkZGhBQsW2FlS0HmsY4LoBAEAAABOYGsIatKkiR588EGtWbNGq1ev1iWXXKIBAwZo06ZNdpYVVPG+ThDHBAEAAACOEGXnnV9++eV+16dMmaLp06dr5cqVat++vU1VBZdvRDadIAAAAMAZbA1BJRUWFmru3LnKzc1VRkZGwHXy8/OVn59vXc/JyQlXeVXmO1kqxwQBAAAAzmD7YIQNGzYoISFBbrdbN998s+bNm6d27doFXHfq1KlKTk62Lk2bNg1ztZVnnSyV6XAAAACAI9gegtq0aaN169bp888/16hRozRs2DD997//DbjuhAkTdPDgQeuSnZ0d5morj5OlAgAAAM5i++5wMTExOuussyRJXbp00RdffKEnn3xSzz33XKl13W633G53uEs8LdaIbDpBAAAAgCPY3gk6mdfr9Tvup7rzlOgEGWNsrgYAAACArZ2gCRMmqF+/fmrWrJkOHTqkWbNmaenSpVq4cKGdZQWVb0S210j5x72KjY60uSIAAACgdrM1BO3du1fXX3+9du3apeTkZHXq1EkLFy7UpZdeamdZQRVXIvTk5h8nBAEAAAA2szUEvfDCC3befVhERrgUFx2pI8cKlVdQqLp2FwQAAADUco47Jqgm8hSfK+hwPhPiAAAAALsRgsLAmhDHmGwAAADAdoSgMIgvPmFqLmOyAQAAANsRgsLANyabThAAAABgP0JQGPjGZNMJAgAAAOxHCAoDOkEAAACAcxCCwsA6JqiAThAAAABgN0JQGPhGZOcxIhsAAACwHSEoDOgEAQAAAM5BCAoDjgkCAAAAnIMQFAZMhwMAAACcgxAUBnSCAAAAAOcgBIUBnSAAAADAOQhBYeDrBOXSCQIAAABsRwgKA4/VCSIEAQAAAHYjBIWBp3hEdh4jsgEAAADbEYLCIL74ZKl0ggAAAAD7EYLCoGQnyBhjczUAAABA7UYICgNfJ+i416ig0GtzNQAAAEDtRggKg/joSOvrPMZkAwAAALYiBIVBVGSE3FFFTzVjsgEAAAB7VSkEZWdn68cff7Sur1q1SmPHjtU//vGPoBVW0/jGZDMhDgAAALBXlULQddddpyVLlkiSdu/erUsvvVSrVq3SXXfdpfvuuy+oBdYU8TFMiAMAAACcoEohaOPGjeratask6Y033lCHDh306aef6rXXXtPMmTODWV+NwbmCAAAAAGeoUgg6duyY3G63JGnx4sX67W9/K0lKT0/Xrl27glddDcK5ggAAAABnqFIIat++vZ599ll98sknWrRokS677DJJ0s6dO1W3bt2gFlhT0AkCAAAAnKFKIeihhx7Sc889p169eunaa69V586dJUnvvvuutZsc/FnHBDEdDgAAALBVVFW+qVevXtq/f79ycnKUkpJiLR85cqTi4+ODVlxN4psOx+5wAAAAgL2q1Ak6cuSI8vPzrQD0/fff64knntCWLVvUoEGDoBZYU3isY4LYHQ4AAACwU5VC0IABA/Tyyy9Lkg4cOKBu3brpscce08CBAzV9+vSgFlhTnDgmiE4QAAAAYKcqhaC1a9eqR48ekqQ333xTDRs21Pfff6+XX35Zf//734NaYE0RXxyCchmMAAAAANiqSiEoLy9PiYmJkqQPP/xQV1xxhSIiInTBBRfo+++/D2qBNYVvd7g8jgkCAAAAbFWlEHTWWWfp7bffVnZ2thYuXKg+ffpIkvbu3aukpKSgFlhT0AkCAAAAnKFKIejee+/V+PHj1aJFC3Xt2lUZGRmSirpC55xzTlALrCmsThDHBAEAAAC2qtKI7CuvvFLdu3fXrl27rHMESVLv3r01aNCgoBVXk1idIKbDAQAAALaqUgiSpEaNGqlRo0b68ccfJUlNmjThRKnl8MTQCQIAAACcoEq7w3m9Xt13331KTk5W8+bN1bx5c9WpU0d/+9vf5PV6g11jjRDvphMEAAAAOEGVOkF33XWXXnjhBT344IO68MILJUn/+c9/NGnSJB09elRTpkwJapE1AZ0gAAAAwBmqFIL++c9/asaMGfrtb39rLevUqZPOOOMM3XLLLYSgAKxOENPhAAAAAFtVaXe4n3/+Wenp6aWWp6en6+effz7tomoiXyeo4LhXxwrZZRAAAACwS5VCUOfOnfX000+XWv7000+rU6dOp11UTeSbDidJeXSDAAAAANtUaXe4hx9+WP3799fixYutcwR99tlnys7O1vvvvx/UAmuKmKgIxURGqKDQq9z840qOi7a7JAAAAKBWqlIn6KKLLtI333yjQYMG6cCBAzpw4ICuuOIKbdq0Sa+88kqwa6wx4jlhKgAAAGC7Kp8nqHHjxqUGIKxfv14vvPCC/vGPf5x2YTWRJyZKB/KOMSYbAAAAsFGVOkGomvji4Qi5dIIAAAAA2xCCwsg3JjuPThAAAABgG0JQGHnoBAEAAAC2q9QxQVdccUW5tx84cOB0aqnxfGOyGZENAAAA2KdSISg5OfmUt19//fWnVVBN5imeDpebTycIAAAAsEulQtBLL70UqjpqBTpBAAAAgP04JiiMOCYIAAAAsB8hKIyYDgcAAADYjxAURnSCAAAAAPsRgsKIThAAAABgP0JQGCW46QQBAAAAdiMEhZFvOhwjsgEAAAD7EILCyMOIbAAAAMB2hKAwimd3OAAAAMB2hKAwsjpBDEYAAAAAbEMICqN4RmQDAAAAtiMEhZGneET20WNeFXqNzdUAAAAAtRMhKIx8nSBJyqMbBAAAANiCEBRG7qgIRUa4JDEhDgAAALALISiMXC7XieOCOFcQAAAAYAtCUJhxriAAAADAXraGoKlTp+r8889XYmKiGjRooIEDB2rLli12lhRy1rmC6AQBAAAAtrA1BC1btkxZWVlauXKlFi1apGPHjqlPnz7Kzc21s6yQohMEAAAA2CvKzjv/4IMP/K7PnDlTDRo00Jo1a9SzZ0+bqgotzhUEAAAA2MvWEHSygwcPSpJSU1MD3p6fn6/8/Hzrek5OTljqCqaE4nMFsTscAAAAYA/HDEbwer0aO3asLrzwQnXo0CHgOlOnTlVycrJ1adq0aZirPH3xVghidzgAAADADo4JQVlZWdq4caNmz55d5joTJkzQwYMHrUt2dnYYKwwOT/HucJwsFQAAALCHI3aHGz16tObPn6/ly5erSZMmZa7ndrvldrvDWFnwxRcPRshlMAIAAABgC1tDkDFGY8aM0bx587R06VK1bNnSznLCwlM8IjuPY4IAAAAAW9gagrKysjRr1iy98847SkxM1O7duyVJycnJiouLs7O0kKETBAAAANjL1mOCpk+froMHD6pXr15KS0uzLnPmzLGzrJCyOkEcEwQAAADYwvbd4WobqxPEdDgAAADAFo6ZDldbMB0OAAAAsBchKMw4TxAAAABgL0JQmNEJAgAAAOxFCAozpsMBAAAA9iIEhRnnCQIAAADsRQgKM4/7RCfI66190/EAAAAAuxGCwswTc2Iq+ZFj7BIHAAAAhBshKMxioyPkchV9nctwBAAAACDsCEFh5nK5rG5QHmOyAQAAgLAjBNkgvnhMNp0gAAAAIPwIQTbwDUfIY0w2AAAAEHaEIBtYnSDGZAMAAABhRwiygXVMEJ0gAAAAIOwIQTaId9MJAgAAAOxCCLIBnSAAAADAPoQgGzAdDgAAALAPIcgG1nQ4zhMEAAAAhB0hyAZ0ggAAAAD7EIJsQCcIAAAAsA8hyAae4k7QYTpBAAAAQNgRgmwQb3WCCEEAAABAuBGCbOAbkZ3LiGwAAAAg7AhBNvCdLDWP3eEAAACAsCME2cA6WSqDEQAAAICwIwTZgBHZAAAAgH0IQTZgRDYAAABgH0KQDTwlOkHGGJurAQAAAGoXQpANfCOyvUbKP+61uRoAAACgdiEE2SAuOtL6OpdzBQEAAABhRQiyQWSEywpCeZwrCAAAAAgrQpBNPG4mxAEAAAB2IATZJL74XEG5TIgDAAAAwooQZBNrTDadIAAAACCsCEE2scZkMxgBAAAACCtCkE18Y7LZHQ4AAAAIL0KQTXydIHaHAwAAAMKLEGQTazACI7IBAACAsCIE2cQ3IjuPY4IAAACAsCIE2YROEAAAAGAPQpBNOCYIAAAAsAchyCZMhwMAAADsQQiyCZ0gAAAAwB6EIJvQCQIAAADsQQiyCZ0gAAAAwB6EIJswHQ4AAACwByHIJgnFu8NxniAAAAAgvAhBNokvPlnqYUIQAAAAEFaEIJt4ineHyysolDHG5moAAACA2oMQZBNfJ+i416ig0GtzNQAAAEDtQQiySXx0pPV1HmOyAQAAgLAhBNkkKjJC7qiipz+XMdkAAABA2BCCbORxnzguCAAAAEB4EIJsFF98wtRcJsQBAAAAYUMIslHJCXEAAAAAwoMQZCPfhDg6QQAAAED4EIJsRCcIAAAACD9CkI2sY4KYDgcAAACEDSHIRtZ0OM4TBAAAAIQNIchGdIIAAACA8CME2SiB8wQBAAAAYUcIslF88WCEw0yHAwAAAMKGEGQjT/GI7DxCEAAAABA2hCAb+TpBuewOBwAAAIQNIchGVieIwQgAAABA2BCCbGR1ghiRDQAAAISNrSFo+fLluvzyy9W4cWO5XC69/fbbdpYTdp4YOkEAAABAuNkagnJzc9W5c2dNmzbNzjJsE++mEwQAAACEW5Sdd96vXz/169evwuvn5+crPz/fup6TkxOKssKGThAAAAAQftXqmKCpU6cqOTnZujRt2tTukk6L1QliOhwAAAAQNtUqBE2YMEEHDx60LtnZ2XaXdFp8naCC414dK/TaXA0AAABQO9i6O1xlud1uud1uu8sIGt90OEnKKyhUcly1yqQAAABAtcRf3TaKiYpQdKRLEscFAQAAAOFCCLKZhwlxAAAAQFjZujvc4cOH9d1331nXt23bpnXr1ik1NVXNmjWzsbLw8cRE6UDeMeXm0wkCAAAAwsHWELR69WpdfPHF1vVx48ZJkoYNG6aZM2faVFV4xRcPR8hldzgAAAAgLGwNQb169ZIxxs4SbOcbk53H7nAAAABAWHBMkM08dIIAAACAsCIE2cw3JjuPE6YCAAAAYUEIspnHXdwJYjACAAAAEBaEIJvRCQIAAADCixBkM44JAgAAAMKLEGQzpsMBAAAA4UUIshmdIAAAACC8CEE2oxMEAAAAhBchyGZ0ggAAAIDwIgTZzDcdjhHZAAAAQHgQgmx2Rp04SdKW3Yd09Bi7xAEAAAChRgiyWfvGSWqUFKvcgkKt+G6/3eUAAAAANR4hyGYRES5d1qGRJGnBxt02VwMAAADUfIQgB+jbvigELd68R8cKvTZXAwAAANRshCAH6NoyVXU9MTqQd0yf/+9nu8sBAAAAajRCkANERrh0abuGkqQPNu2yuRoAAACgZiMEOUTf4uOCFm7aI6/X2FwNAAAAUHMRghziwlb1lOiO0r5D+Vr7wy92lwMAAADUWIQgh4iJilDvtg0kSR8wJQ4AAAAIGUKQg1zWIU1S0ahsY9glDgAAAAgFQpCDXHR2fcVGR2jHgSPatDPH7nIAAACAGokQ5CBxMZHqdTa7xAEAAAChRAhymH4di6bELdjIqGwAAAAgFAhBDnNxegNFR7q0dV+uvtt7yO5yAAAAgBqHEOQwSbHR6n5WPUnsEgcAAACEAiHIgS7r4NsljhAEAAAABBshyIEy2zZUhEvatDNH2T/n2V0OAAAAUKMQghyoboJb3VrWlcQucQAAAECwEYIcyrdL3AebCEEAAABAMBGCHKpv+6IQtOb7X7Q356jN1QAAAAA1ByHIoRolx+qcZnUkSQvpBgEAAABBQwhysMvas0scAAAAEGyEIAfzHRe08n8/65fcApurAQAAAGoGQpCDNa/rUdu0JBV6jRZt3mN3OQAAAECNQAhyuH7F3aCFjMoGAAAAgoIQ5HC+XeI++Xa/Dh09ZnM1AAAAQPVHCHK41g0SdGY9jwoKvVqyZZ/d5QAAAADVHiHI4Vwu14kTp27cZXM1AAAAQPVHCKoGfCFoydf7dPRYoc3VAAAAANUbIaga6HhGss6oE6cjxwq1/Bt2iQMAAABOByGoGnC5XOrrO3EqU+IAAACA00IIqiZ8u8Qt3rxHBce9NlcDAAAAVF+EoGqiS/MU1UtwK+focX32v5/sLgcAAACotghB1URkhEt92jeUxC5xAAAAwOkgBFUjlxUfF7Tov7tV6DU2VwMAAABUT4SgaiSjVV0lxUZp/+ECrd7+s93lAAAAANUSIagaiY6MUGa74l3iNrFLHAAAAFAVhKBqpl+HNEnSwo27ZQy7xAEAAACVRQiqZnq0rqf4mEjtPHhUX/140O5yAAAAgGqHEFTNxEZH6uI2DSSxSxwAAABQFYSgash34tQP2CUOAAAAqDRCUDV0cXoDxURFaNv+XH2z57Dd5QAAAADVCiGoGkpwR6ln63qSOHEqAAAAUFlRdheAqunbvpEWb96r+V/tVPfW9eSOilB0ZIRiooou0ZEuuSMjFR3lUkxkhKIiybsAAACARAiqtjLbNlRkhEvf7j2swdM/PeX6ES6dCEmRRYEpwnXidpfLFfD7XK7SX7tUet1A3x7oJ5a8n1K3u8q+akp8YSQZY4r/9S02MqbE9QDHSgV6jNZjKvk4A1ZespbQHod14jEEuu3EQqNy1gtQY3mHj7lcJx530de+5SeWnbzeqe7zVPcnFf2sk+/PVXSDfP9Yy6z78f1843ddJvDtpyPgNhNwxQotOqVAj+Xkx1lymy95PRgC/R64SrwWJ9Y7cc33u3hybYG244r+XpaqpSpPZgiU3BJPfl4CvreV85528uM++SGWfJ/zve+pxDJjyn6eT66xzOsBHo9dynpuAj6XAf4fKPl/QMDlQa3Pt8z/d6TkeqVeX9eJ98+y3vNcJb7PVfQw5S0u3ve17zF5vcVPhTHyVuBxBv7/ONB6ZW/jgR5XIGW9J/hfL11foP+H/LYFV+l1gqGi/4ee7OT/CwNtBydus+cX7OTfn1L/t6v09lzytfJ7Hyq6sdT/S75tMiYyQh/9qVeoHkpIEIKqqRRPjG7PbK231+1UwXGvCo57dayw6N+CwqJLyV9sr5Hyj3uVf9xrX9EAAACocdxR1W+PI0JQNTb6ktYafUnrgLcZY3Tca/yDkRWWipb70vuJ7yn+96SfU3LZqT91LnsFv0+Ey7mt5P361vX7hMzl/+mFyvhkreQHL2Xdd6DOim/98j64Oa1P+cu63ajUJ2+BPhEt79P6QMr79LeoLuP//JzcXbOW+3+6V5HuX6BPv07+BEmlPnXyv7+S9VSkO1Hep3GVUZHXq+zbqv65c6num/y39ZNvK/k9p6Ps3/XAn+CevL0GqrHUp90qvd2U+t0PUFPJ2+xqWFTmeSn6N0A3NtDPLfX4/bdzqxvq93xKJd/3Sn6qG+i+SnVNS91v1bfXU71XnvoVO9X/Qb5/TzyGk5+TUl+rqKYIv/8zXOXWWe7vc4AaA10P1J09uWNlrWn8t5mT31+NjCKsx+JShOvE6170ddFtEa4Tj6usx1nVvQUq8thU/PhO3gbL69gE6jyc/H+QX5dTgZ/Dqgi8vZbufpW+5aSfE+DnlnVrVf87qPpvpf/9lrs3QRnv6yXfzgN2LwO8H0WU8/vlVISgGsrlcik60qXoyAjFx9hdDQAAAOAc1a93BQAAAACngRAEAAAAoFYhBAEAAACoVQhBAAAAAGoVQhAAAACAWoUQBAAAAKBWcUQImjZtmlq0aKHY2Fh169ZNq1atsrskAAAAADWU7SFozpw5GjdunCZOnKi1a9eqc+fO6tu3r/bu3Wt3aQAAAABqINtD0OOPP64bb7xRI0aMULt27fTss88qPj5eL774ot2lAQAAAKiBbA1BBQUFWrNmjTIzM61lERERyszM1GeffVZq/fz8fOXk5PhdAAAAAKAybA1B+/fvV2FhoRo2bOi3vGHDhtq9e3ep9adOnark5GTr0rRp03CVCgAAAKCGsH13uMqYMGGCDh48aF2ys7PtLgkAAABANRNl553Xq1dPkZGR2rNnj9/yPXv2qFGjRqXWd7vdcrvd4SoPAAAAQA1kaycoJiZGXbp00UcffWQt83q9+uijj5SRkWFjZQAAAABqKls7QZI0btw4DRs2TOedd566du2qJ554Qrm5uRoxYsQpv9cYI0kMSAAAAABqOV8m8GWE8tgegq6++mrt27dP9957r3bv3q1f/epX+uCDD0oNSwjk0KFDksSABAAAAACSijJCcnJyueu4TEWikkN5vV7t3LlTiYmJcrlcttaSk5Ojpk2bKjs7W0lJSbbWguqH7Qeng+0Hp4PtB6eD7QdVFYptxxijQ4cOqXHjxoqIKP+oH9s7QacjIiJCTZo0sbsMP0lJSbwJoMrYfnA62H5wOth+cDrYflBVwd52TtUB8qlWI7IBAAAA4HQRggAAAADUKoSgIHG73Zo4cSLnMUKVsP3gdLD94HSw/eB0sP2gquzedqr1YAQAAAAAqCw6QQAAAABqFUIQAAAAgFqFEAQAAACgViEEAQAAAKhVCEFBMm3aNLVo0UKxsbHq1q2bVq1aZXdJcKDly5fr8ssvV+PGjeVyufT222/73W6M0b333qu0tDTFxcUpMzNT3377rT3FwlGmTp2q888/X4mJiWrQoIEGDhyoLVu2+K1z9OhRZWVlqW7dukpISNDgwYO1Z88emyqGk0yfPl2dOnWyTkqYkZGhBQsWWLez7aCiHnzwQblcLo0dO9ZaxvaD8kyaNEkul8vvkp6ebt1u1/ZDCAqCOXPmaNy4cZo4caLWrl2rzp07q2/fvtq7d6/dpcFhcnNz1blzZ02bNi3g7Q8//LD+/ve/69lnn9Xnn38uj8ejvn376ujRo2GuFE6zbNkyZWVlaeXKlVq0aJGOHTumPn36KDc311rn9ttv17///W/NnTtXy5Yt086dO3XFFVfYWDWcokmTJnrwwQe1Zs0arV69WpdccokGDBigTZs2SWLbQcV88cUXeu6559SpUye/5Ww/OJX27dtr165d1uU///mPdZtt24/BaevatavJysqyrhcWFprGjRubqVOn2lgVnE6SmTdvnnXd6/WaRo0amUceecRaduDAAeN2u83rr79uQ4Vwsr179xpJZtmyZcaYom0lOjrazJ0711pn8+bNRpL57LPP7CoTDpaSkmJmzJjBtoMKOXTokGndurVZtGiRueiii8xtt91mjOG9B6c2ceJE07lz54C32bn90Ak6TQUFBVqzZo0yMzOtZREREcrMzNRnn31mY2WobrZt26bdu3f7bUvJycnq1q0b2xJKOXjwoCQpNTVVkrRmzRodO3bMb/tJT09Xs2bN2H7gp7CwULNnz1Zubq4yMjLYdlAhWVlZ6t+/v992IvHeg4r59ttv1bhxY5155pkaOnSofvjhB0n2bj9RIf3ptcD+/ftVWFiohg0b+i1v2LChvv76a5uqQnW0e/duSQq4LfluAyTJ6/Vq7NixuvDCC9WhQwdJRdtPTEyM6tSp47cu2w98NmzYoIyMDB09elQJCQmaN2+e2rVrp3Xr1rHtoFyzZ8/W2rVr9cUXX5S6jfcenEq3bt00c+ZMtWnTRrt27dLkyZPVo0cPbdy40dbthxAEANVMVlaWNm7c6LdPNXAqbdq00bp163Tw4EG9+eabGjZsmJYtW2Z3WXC47Oxs3XbbbVq0aJFiY2PtLgfVUL9+/ayvO3XqpG7duql58+Z64403FBcXZ1td7A53murVq6fIyMhSUyz27NmjRo0a2VQVqiPf9sK2hPKMHj1a8+fP15IlS9SkSRNreaNGjVRQUKADBw74rc/2A5+YmBidddZZ6tKli6ZOnarOnTvrySefZNtBudasWaO9e/fq3HPPVVRUlKKiorRs2TL9/e9/V1RUlBo2bMj2g0qpU6eOzj77bH333Xe2vv8Qgk5TTEyMunTpoo8++sha5vV69dFHHykjI8PGylDdtGzZUo0aNfLblnJycvT555+zLUHGGI0ePVrz5s3Txx9/rJYtW/rd3qVLF0VHR/ttP1u2bNEPP/zA9oOAvF6v8vPz2XZQrt69e2vDhg1at26ddTnvvPM0dOhQ62u2H1TG4cOHtXXrVqWlpdn6/sPucEEwbtw4DRs2TOedd566du2qJ554Qrm5uRoxYoTdpcFhDh8+rO+++866vm3bNq1bt06pqalq1qyZxo4dq/vvv1+tW7dWy5Ytdc8996hx48YaOHCgfUXDEbKysjRr1iy98847SkxMtPaVTk5OVlxcnJKTk/WHP/xB48aNU2pqqpKSkjRmzBhlZGToggsusLl62G3ChAnq16+fmjVrpkOHDmnWrFlaunSpFi5cyLaDciUmJlrHHvp4PB7VrVvXWs72g/KMHz9el19+uZo3b66dO3dq4sSJioyM1LXXXmvv+09IZ8/VIk899ZRp1qyZiYmJMV27djUrV660uyQ40JIlS4ykUpdhw4YZY4rGZN9zzz2mYcOGxu12m969e5stW7bYWzQcIdB2I8m89NJL1jpHjhwxt9xyi0lJSTHx8fFm0KBBZteuXfYVDce44YYbTPPmzU1MTIypX7++6d27t/nwww+t29l2UBklR2Qbw/aD8l199dUmLS3NxMTEmDPOOMNcffXV5rvvvrNut2v7cRljTGhjFgAAAAA4B8cEAQAAAKhVCEEAAAAAahVCEAAAAIBahRAEAAAAoFYhBAEAAACoVQhBAAAAAGoVQhAAAACAWoUQBAAAAKBWIQQBAGotl8ult99+2+4yAABhRggCANhi+PDhcrlcpS6XXXaZ3aUBAGq4KLsLAADUXpdddpleeuklv2Vut9umagAAtQWdIACAbdxutxo1auR3SUlJkVS0q9r06dPVr18/xcXF6cwzz9Sbb77p9/0bNmzQJZdcori4ONWtW1cjR47U4cOH/dZ58cUX1b59e7ndbqWlpWn06NF+t+/fv1+DBg1SfHy8WrdurXfffTe0DxoAYDtCEADAse655x4NHjxY69ev19ChQ3XNNddo8+bNkqTc3Fz17dtXKSkp+uKLLzR37lwtXrzYL+RMnz5dWVlZGjlypDZs2KB3331XZ511lt99TJ48WUOGDNFXX32lX//61xo6dKh+/vnnsD5OAEB4uYwxxu4iAAC1z/Dhw/Xqq68qNjbWb/lf//pX/fWvf5XL5dLNN9+s6dOnW7ddcMEFOvfcc/XMM8/o+eef15133qns7Gx5PB5J0vvvv6/LL79cO3fuVMOGDXXGGWdoxIgRuv/++wPW4HK5dPfdd+tvf/ubpKJglZCQoAULFnBsEgDUYBwTBACwzcUXX+wXciQpNTXV+jojI8PvtoyMDK1bt06StHnzZnXu3NkKQJJ04YUXyuv1asuWLXK5XNq5c6d69+5dbg2dOnWyvvZ4PEpKStLevXur+pAAANUAIQgAYBuPx1Nq97RgiYuLq9B60dHRftddLpe8Xm8oSgIAOATHBAEAHGvlypWlrrdt21aS1LZtW61fv165ubnW7StWrFBERITatGmjxMREtWjRQh999FFYawYAOB+dIACAbfLz87V7926/ZVFRUapXr54kae7cuTrvvPPUvXt3vfbaa1q1apVeeOEFSdLQoUM1ceJEDRs2TJMmTdK+ffs0ZswY/f73v1fDhg0lSZMmTdLNN9+sBg0aqF+/fjp06JBWrFihMWPGhPeBAgAchRAEALDNBx98oLS0NL9lbdq00ddffy2paHLb7NmzdcsttygtLU2vv/662rVrJ0mKj4/XwoULddttt+n8889XfHy8Bg8erMcff9z6WcOGDdPRo0f1//7f/9P48eNVr149XXnlleF7gAAAR2I6HADAkVwul+bNm6eBAwfaXQoAoIbhmCAAAAAAtQohCAAAAECtwjFBAABHYm9tAECo0AkCAAAAUKsQggAAAADUKoQgAAAAALUKIQgAAABArUIIAgAAAFCrEIIAAAAA1CqEIAAAAAC1CiEIAAAAQK3y/wFiYEtGznRyZAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the best configuration\n",
    "best_obs = best_config['Observations']\n",
    "best_lr = best_config['Learning Rate']\n",
    "best_loss = best_config['Loss Function']\n",
    "_, _, _, epoch_losses = train_model(best_obs, best_lr, best_loss, epochs=50)\n",
    "\n",
    "# Plot Loss Over Epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epoch_losses)\n",
    "plt.title('Loss Over Epochs (Best Configuration)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Our experiments provided valuable insights into the impact of hyperparameters on the performance of the linear regression model. Below are the key observations regarding the choice of epochs and other parameters:\n",
    "\n",
    "### Observations on Epochs\n",
    "Although we increased the number of epochs from **7** to **50** in the best-performing configuration, our results indicate that training for just **7 epochs** is sufficient to achieve optimal performance. Specifically, from epoch 5 onward, the loss values stabilized, fluctuating only between **0.1687** and **0.1695**. This stabilization suggests that further training does not significantly improve model performance.\n",
    "\n",
    "This behavior raises concerns about potential overfitting, as continued training beyond this point may lead to unnecessary computations without enhancing accuracy. To mitigate this risk, implementing **early stopping** could be beneficial. Early stopping would allow us to terminate training once the loss stabilizes, thereby preventing overfitting and optimizing computational resources.\n",
    "\n",
    "### Key Insights\n",
    "1. **Impact of Observations**: Increasing the number of observations generally leads to improved performance metrics, as evidenced by higher R-squared values.\n",
    "2. **Learning Rate Sensitivity**: The selection of learning rate significantly influences convergence speed and final model accuracy. Lower learning rates can result in slower convergence or underfitting.\n",
    "3. **Loss Function Comparison**: The Huber loss function often outperformed mean squared error in scenarios where noise was present, demonstrating its robustness against outliers.\n",
    "4. **Model Stability**: While we did not change the target function in this experiment, the model consistently learned from a defined linear relationship. This indicates that it can effectively approximate known functions with appropriate hyperparameter tuning.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In summary, this notebook successfully illustrates how various hyperparameters influence the performance of a simple linear regression model using TensorFlow. By systematically varying observation counts, learning rates, and loss functions, we have gained critical insights into optimizing model training for linear regression tasks.\n",
    "\n",
    "Future work could involve exploring more complex models or datasets that include inherent outliers to further validate these findings and enhance our understanding of model behavior under different conditions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
